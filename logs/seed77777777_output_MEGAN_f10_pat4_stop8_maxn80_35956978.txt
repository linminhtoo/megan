Project root set as /home/linmin001/megan_77777777
2021-05-01 08:56:21,447 - src - INFO - Setting random seed to 77777777
2021-05-01 08:56:24,938 - src.utils.dispatch_utils - INFO - Parsed gin config from /home/linmin001/megan_77777777/configs/uspto_50k.gin
2021-05-01 08:56:24,943 - src.utils.dispatch_utils - INFO - gin-bound config values:
get_dataset.dataset_key = uspto_50k
train_megan.featurizer_key = megan_16_bfs_randat
train_megan.max_n_epochs = 80
train_megan.train_samples_per_epoch = -1
train_megan.valid_samples_per_epoch = -1
train_megan.batch_size = 4
train_megan.learning_rate = 0.0001
train_megan.gen_lr_factor = 0.1
train_megan.gen_lr_patience = 4
train_megan.early_stopping = 8
train_megan.start_epoch = 0
train_megan.megan_warmup_epochs = 1
Megan.reaction_type_given = False
Megan.bond_emb_dim = 32
Megan.hidden_dim = 1024
Megan.stateful = True
Megan.n_reaction_types = 10
Megan.reaction_type_emb_dim = 16
Megan.atom_feature_keys = ['is_supernode', 'atomic_num', 'formal_charge', 'chiral_tag', 'num_explicit_hs', 'is_aromatic']
Megan.bond_feature_keys = ['bond_type', 'bond_stereo']
MeganEncoder.n_encoder_conv = 6
MeganEncoder.enc_residual = True
MeganEncoder.enc_dropout = 0.0
MeganDecoder.n_decoder_conv = 2
MeganDecoder.dec_residual = True
MeganDecoder.n_fc = 2
MeganDecoder.atom_fc_hidden_dim = 128
MeganDecoder.bond_fc_hidden_dim = 128
MeganDecoder.bond_atom_dim = 128
MeganDecoder.dec_dropout = 0.0
MultiHeadGraphConvLayer.att_heads = 8
MultiHeadGraphConvLayer.att_dim = 128
2021-05-01 08:56:24,950 - __main__ - INFO - Creating model...
2021-05-01 08:56:28,180 - __main__ - INFO - Loading data...
2021-05-01 08:56:28,180 - __main__ - INFO - Training for maximum of 80 epochs...
2021-05-01 08:56:28,181 - __main__ - INFO - Loading data
2021-05-01 08:56:29,033 - __main__ - INFO - Training on chunk of 39643 training samples and 4980 valid samples
2021-05-01 08:56:29,033 - __main__ - INFO - Starting training on epoch 1 with Learning Rate=0.0 (1 warmup epochs)
2021-05-01 08:56:32,309 - __main__ - WARNING - Exception while running batch: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 1.77 GiB already allocated; 7.75 MiB free; 14.19 MiB cached)
